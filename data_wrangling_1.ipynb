{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pranaypk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\pranaypk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Data Handling Libraries\n",
    "import xmltodict\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "#URL request library\n",
    "import requests\n",
    "\n",
    "#Path Setup Libraries\n",
    "import os, pathlib, glob\n",
    "from pathlib import Path\n",
    "\n",
    "#NLP LIbraries\n",
    "import nltk, re\n",
    "from nltk import RegexpTokenizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, pos_tag, word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#XML Parsing Libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#To work with stop words and puctuations\n",
    "import string\n",
    "\n",
    "#Tranformers for Medical Named Entity Recognition and Word Embedding Vectors from huggingface\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "#AutoModel for you word embeddings model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#PyTorch for running the ClinicalBert\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url for the medical test from n2c2\n",
    "url_text = \"\" \n",
    "#url for test data from n2c2\n",
    "url_test = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(url_text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    xml_data = response.content\n",
    "    root = ET.fromstring(xml_data)\n",
    "\n",
    "    df = pd.DataFrame(columns=['doc_id', 'text'])\n",
    "    for doc in root.findall(\".//doc\"):\n",
    "        doc_id = doc.get(\"id\")\n",
    "        text = doc.find(\"text\").text.strip()\n",
    "        doc_id = int(doc_id)\n",
    "        df.loc[len(df)] = [doc_id, text]\n",
    "else:\n",
    "    print(f\"Failed to fetch data\")\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Stop workds\n",
    "stop = stopwords.words('english')\n",
    "stop = sorted(stop)\n",
    "\n",
    "#Tokenizing the text\n",
    "df['text'] = df['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "df['text'] = df['text'].apply(lambda x: [token for token in x if token.lower() not in stop])\n",
    "\n",
    "# Remove punctuations\n",
    "df['text'] = df['text'].apply(lambda x: [token for token in x if token not in string.punctuation])\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-trained model and tokenizer to get Named Entities from the  text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Clinical_event', 'Dosage', 'Detailed_description', 'Therapeutic_procedure', 'Duration', 'Sign_symptom', 'Sex', 'Disease_disorder', 'History', 'Time', 'Coreference', 'Age', 'Medication', 'Lab_value', 'Diagnostic_procedure', 'Nonbiological_location']\n"
     ]
    }
   ],
   "source": [
    "#Getting all the unique entity names from one sample mapped NER's\n",
    "sample_text = ' '.join(df.at[0,'text'])\n",
    "data = pipe(sample_text)\n",
    "unique_words = set()\n",
    "\n",
    "for item in data:\n",
    "    if 'entity_group' in item:\n",
    "        unique_words.add(item['entity_group'])\n",
    "\n",
    "# Convert the set of unique words back to a list\n",
    "unique_words_list = list(unique_words)\n",
    "\n",
    "#Unique Entities\n",
    "print(unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the Named Entity for tokens of each row (each record)\n",
    "#Loops through each item to get entity group, score, and the actual word\n",
    "def disease_named_entity_recognition(row_data):\n",
    "    table_variables = {}\n",
    "    string_text = ' '.join(row_data)\n",
    "    ner = pipe(string_text)\n",
    "    for i in unique_words_list:\n",
    "        filtered_ner = [item for item in ner if item.get('entity_group') == i]\n",
    "        sorted_ner = sorted(filtered_ner, key=lambda x: x['score'], reverse=True)\n",
    "        just_name = set(item.get('word') for item in sorted_ner)\n",
    "        table_variables[i] = just_name\n",
    "    return table_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes time! 5 min in my snail paced laptop!!\n",
    "\n",
    "#Apply the NER function to each medical text\n",
    "df['extracted_diseases'] = df['text'].apply(disease_named_entity_recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping thorpugh the unquie entity list to create new columns in out dataFrame\n",
    "for group in unique_words_list:\n",
    "    df[group] = None\n",
    "\n",
    "#Split our dictiory in the extracted variables column into the multiple columns - The empty ones we just created!\n",
    "for i, row in df.iterrows():\n",
    "    extracted_diseases = row['extracted_diseases']\n",
    "    for group in unique_words_list:\n",
    "        entity_set = list(extracted_diseases.get(group, set()))\n",
    "        df.at[i, group] = entity_set\n",
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract age from the age text - so we can actually perform an analysis on that\n",
    "\n",
    "def extract_integer_age(age_set):\n",
    "    for phrase in age_set:\n",
    "        age_match = re.search(r'\\d+', phrase)\n",
    "        if age_match:\n",
    "            return int(age_match.group())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'male', 'NA'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply that extract age function through all rows\n",
    "df['Age'] = df['Age'].apply(extract_integer_age)\n",
    "\n",
    "#Clean the Sex column and convert into a 2 level categorical value\n",
    "df['Sex'] = df['Sex'].apply(\"\".join)\n",
    "\n",
    "#Mapping and pattern for various sex values\n",
    "gender_patterns = [\n",
    "    (r'\\bwoman\\b|\\bfemale\\b', 'female'),\n",
    "    (r'\\bman\\b|\\bmale\\b', 'male'),\n",
    "]\n",
    "\n",
    "for pattern, replacement in gender_patterns:\n",
    "    df['Sex'] = df['Sex'].apply(lambda x: re.sub(pattern, replacement, x))\n",
    "\n",
    "#Mapping again!\n",
    "gender_mapping = {\n",
    "    'female': 'female',\n",
    "    'male': 'male',\n",
    "    'woman': 'female',\n",
    "    'man': 'male',\n",
    "    'gentleman': 'male',\n",
    "    'lady': 'female'\n",
    "}\n",
    "\n",
    "# Apply the mapping and if you have no matches, NA it is!\n",
    "df['Sex'] = df['Sex'].apply(lambda x: gender_mapping.get(x, 'NA'))\n",
    "\n",
    "#Check the values in the Sex Column now (Should be male, female and NA!)\n",
    "df['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer and Model from the Bio_ClinicalBert model from HuggingFace\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "#Function to get the word embedding vector. Referenced from Carly's example!\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    # Tokenize the word and convert to IDs\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Pass the input through the model to get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Extract embeddings from the output\n",
    "    word_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return word_embedding[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pranaypk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pranaypk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pranaypk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\pranaypk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "############## THIS TAKES FOREEEEEVEEEER TO RUN ############## 40 min in my computer!!######\n",
    "\n",
    "#Loopthrough the text in each of those columns and get an array of word embeddings\n",
    "for col in df[['Sign_symptom', 'Dosage', 'Medication', 'Disease_disorder']].columns:\n",
    "    new_col = f'{col}_mean_embedding'\n",
    "    df[new_col] = df[col].apply(lambda word_list: np.mean([get_word_embedding(word) for word in word_list], axis=0))\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Averaging the word embedd values, to get an overall score\n",
    "\n",
    "for col in df[['Sign_symptom_mean_embedding', 'Dosage_mean_embedding', 'Medication_mean_embedding', 'Disease_disorder_mean_embedding']].columns:\n",
    "    df[col] = df[col].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the Values so you can use it to predict the final Cateogory!\n",
    "\n",
    "columns_to_scale = ['Sign_symptom_mean_embedding', 'Dosage_mean_embedding', 'Medication_mean_embedding', 'Disease_disorder_mean_embedding']\n",
    "\n",
    "def scale_and_fill_na_with_mean(dataframe):\n",
    "    column_means = dataframe.mean()\n",
    "    scaled_df = (dataframe - column_means) / dataframe.std()\n",
    "    scaled_and_filled_df = scaled_df.fillna(column_means)\n",
    "    return scaled_and_filled_df\n",
    "\n",
    "df[columns_to_scale] = scale_and_fill_na_with_mean(df[columns_to_scale])\n",
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url_test)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    xml_data = response.content\n",
    "    root = ET.fromstring(xml_data)\n",
    "    data = {}\n",
    "    for disease_elem in root.findall(\".//disease\"):\n",
    "        disease_name = disease_elem.get(\"name\")\n",
    "        doc_data = {}\n",
    "\n",
    "        for doc_elem in disease_elem.findall(\".//doc\"):\n",
    "            doc_id = doc_elem.get(\"id\")\n",
    "            judgment = doc_elem.get(\"judgment\", \"Q\")  # Default to \"Q\" if no judgment attribute\n",
    "            doc_data[doc_id] = judgment\n",
    "\n",
    "        data[disease_name] = doc_data\n",
    "\n",
    "    test_df = pd.DataFrame(data)\n",
    "    test_df['doc_id'] = test_df.index\n",
    "else:\n",
    "    print(f\"Failed to fetch data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner Join it with the main dataFrame with doc_id as the key\n",
    "\n",
    "df[['doc_id']] = df[['doc_id']].astype(int)\n",
    "test_df[['doc_id']] = test_df[['doc_id']].astype(int)\n",
    "merged_df = pd.merge(df, test_df, on=\"doc_id\" ,how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Final Data Set. Yaaaayyyyyyy!\n",
    "# Get columns that are necassary\n",
    "columns_needed = ['doc_id','Age','Sex','Sign_symptom_mean_embedding',\n",
    "       'Dosage_mean_embedding', 'Medication_mean_embedding',\n",
    "       'Disease_disorder_mean_embedding', 'Asthma', 'CAD', 'CHF', 'Depression',\n",
    "       'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia',\n",
    "       'Hypertension', 'Hypertriglyceridemia', 'OA', 'Obesity', 'OSA', 'PVD',\n",
    "       'Venous Insufficiency']\n",
    "\n",
    "final_df = merged_df[columns_needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Sign_symptom_mean_embedding</th>\n",
       "      <th>Dosage_mean_embedding</th>\n",
       "      <th>Medication_mean_embedding</th>\n",
       "      <th>Disease_disorder_mean_embedding</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>CAD</th>\n",
       "      <th>CHF</th>\n",
       "      <th>...</th>\n",
       "      <th>GERD</th>\n",
       "      <th>Gout</th>\n",
       "      <th>Hypercholesterolemia</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Hypertriglyceridemia</th>\n",
       "      <th>OA</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>OSA</th>\n",
       "      <th>PVD</th>\n",
       "      <th>Venous Insufficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.135360</td>\n",
       "      <td>0.160407</td>\n",
       "      <td>-0.359768</td>\n",
       "      <td>0.300799</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.109368</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.006882</td>\n",
       "      <td>0.240273</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.052523</td>\n",
       "      <td>-0.321604</td>\n",
       "      <td>-0.332040</td>\n",
       "      <td>-0.226017</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.301072</td>\n",
       "      <td>-0.042164</td>\n",
       "      <td>-0.141801</td>\n",
       "      <td>-0.472457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1.159010</td>\n",
       "      <td>1.006911</td>\n",
       "      <td>-0.619233</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1228</td>\n",
       "      <td>52.0</td>\n",
       "      <td>male</td>\n",
       "      <td>-0.259325</td>\n",
       "      <td>0.214659</td>\n",
       "      <td>-0.494626</td>\n",
       "      <td>-0.783657</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1230</td>\n",
       "      <td>62.0</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.126740</td>\n",
       "      <td>0.376808</td>\n",
       "      <td>-0.102933</td>\n",
       "      <td>-0.488200</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>1231</td>\n",
       "      <td>62.0</td>\n",
       "      <td>female</td>\n",
       "      <td>-0.073993</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.704967</td>\n",
       "      <td>-0.556493</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1241</td>\n",
       "      <td>63.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.140653</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>-0.828622</td>\n",
       "      <td>-1.316808</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1247</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>-0.822399</td>\n",
       "      <td>-0.123576</td>\n",
       "      <td>-0.830338</td>\n",
       "      <td>-1.290774</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id   Age     Sex  Sign_symptom_mean_embedding  Dosage_mean_embedding  \\\n",
       "0         3  57.0  female                    -0.135360               0.160407   \n",
       "1         5  64.0    male                     0.109368              -0.007173   \n",
       "2         7  56.0  female                    -0.052523              -0.321604   \n",
       "3         8   NaN  female                    -0.301072              -0.042164   \n",
       "4         9   NaN      NA                    -1.159010               1.006911   \n",
       "..      ...   ...     ...                          ...                    ...   \n",
       "502    1228  52.0    male                    -0.259325               0.214659   \n",
       "503    1230  62.0  female                    -0.126740               0.376808   \n",
       "504    1231  62.0  female                    -0.073993              -0.007173   \n",
       "505    1241  63.0  female                     0.140653               0.341176   \n",
       "506    1247  66.0      NA                    -0.822399              -0.123576   \n",
       "\n",
       "     Medication_mean_embedding  Disease_disorder_mean_embedding Asthma  CAD  \\\n",
       "0                    -0.359768                         0.300799      Y    Y   \n",
       "1                    -0.006882                         0.240273      N    Y   \n",
       "2                    -0.332040                        -0.226017      N    N   \n",
       "3                    -0.141801                        -0.472457    NaN  NaN   \n",
       "4                    -0.619233                        -0.006763      Y    N   \n",
       "..                         ...                              ...    ...  ...   \n",
       "502                  -0.494626                        -0.783657      N    Y   \n",
       "503                  -0.102933                        -0.488200      N    Y   \n",
       "504                  -0.704967                        -0.556493      Y  NaN   \n",
       "505                  -0.828622                        -1.316808      N    Y   \n",
       "506                  -0.830338                        -1.290774      N    N   \n",
       "\n",
       "     CHF  ... GERD Gout Hypercholesterolemia Hypertension  \\\n",
       "0      Y  ...    N    N                    Y            Y   \n",
       "1    NaN  ...  NaN    N                    Y            Y   \n",
       "2      Y  ...    N    N                    N            N   \n",
       "3      Y  ...    Y    N                    N          NaN   \n",
       "4    NaN  ...    Y    N                    Y            Y   \n",
       "..   ...  ...  ...  ...                  ...          ...   \n",
       "502    Y  ...    N    N                    N            N   \n",
       "503    N  ...    N    N                    Y            N   \n",
       "504    Y  ...  NaN    N                    Y            Y   \n",
       "505    N  ...    N    N                    Y            Y   \n",
       "506    Y  ...    Y    N                    N            Y   \n",
       "\n",
       "    Hypertriglyceridemia   OA Obesity  OSA  PVD Venous Insufficiency  \n",
       "0                      N    Y     NaN    Y    Y                    N  \n",
       "1                    NaN    N     NaN    N  NaN                    N  \n",
       "2                      N    N       Y    N    N                    N  \n",
       "3                      N    N     NaN    Y    N                    N  \n",
       "4                      N  NaN     NaN  NaN    N                    N  \n",
       "..                   ...  ...     ...  ...  ...                  ...  \n",
       "502                    N    N     NaN    N    N                    N  \n",
       "503                    N    N       N    N    N                    N  \n",
       "504                    N    Y       Y    Y    N                  NaN  \n",
       "505                    N    N     NaN    N    N                  NaN  \n",
       "506                    N    N       Y    N    N                    N  \n",
       "\n",
       "[507 rows x 23 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r'../data/output_finaldf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed next to the Data_model file to run the model with this cleaned and wrangled data!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
